{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# W207 Final Project\n",
    "Erika, Jen Jen, Geoff, Leslie\n",
    "\n",
    "(In Python 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As of 3/35\n",
    "\n",
    "Outline:\n",
    "\n",
    "* Data Pre-Processing  \n",
    "* Simple Feature Selection\n",
    "* Basline Models\n",
    "* Possible Approaches\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 1 Loading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Import Libraries ##\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pandas import *\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of people who got pizza: 0.241\n",
      "\n",
      "Size of the normalized Data: (3040, 32)\n",
      "\n",
      "normalized data columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "## Get Data ##\n",
    "\n",
    "#reference on data: https://www.kaggle.com/c/random-acts-of-pizza/data\n",
    "# pull in the training and test data\n",
    "with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/train.json', encoding='utf-8') as data_file:\n",
    "#with open('/home/levi/Documents/W207_Proj/data/train.json', encoding='utf-8') as data_file:\n",
    "    trainData = json.loads(data_file.read())   \n",
    "with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/test.json', encoding='utf-8') as data_file:\n",
    "#with open('/home/levi/Documents/W207_Proj/data/train.json', encoding='utf-8') as data_file:\n",
    "    testData = json.loads(data_file.read())    \n",
    "\n",
    "# create a dev data set \n",
    "devData = trainData[0:1000]\n",
    "trainData = trainData[1000:]\n",
    "\n",
    "# baseline info\n",
    "print(\"Percent of people who got pizza:\", round(sum(trainLabel)/len(trainLabel),3))\n",
    "\n",
    "# show how the data looks in its original format\n",
    "#pprint(\"data in json format:\")\n",
    "#pprint(trainData[1])\n",
    "\n",
    "# create a normalized view\n",
    "allTData = json_normalize(trainData)\n",
    "print(\"\\nSize of the normalized Data:\", allTData.shape)\n",
    "print(\"\\nnormalized data columns:\", list(allTData))\n",
    "\n",
    "allDData = json_normalize(devData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3040, 1)\n"
     ]
    }
   ],
   "source": [
    "## Create subsets of data for analysis ###\n",
    "\n",
    "# create a flat dataset without the subreddits list\n",
    "flatData = allTData.drop('requester_subreddits_at_request', 1)\n",
    "# create a separate dataset with just subreddits, indexed on request id\n",
    "# we can creata a count vector on the words, run Naive Bayes against it, \n",
    "# and add the probabilities to our flat dataset\n",
    "subredTData = allTData[['request_id','requester_subreddits_at_request']]\n",
    "subredTData.set_index('request_id', inplace=True)\n",
    "\n",
    "subredDData= allDData[['request_id','requester_subreddits_at_request']]\n",
    "subredDData.set_index('request_id', inplace=True)\n",
    "\n",
    "# our training labels\n",
    "trainLabel = allTData['requester_received_pizza']\n",
    "\n",
    "devLabel = allDData['requester_received_pizza']\n",
    "\n",
    "# what do these look like?\n",
    "#print(list(flatData))\n",
    "print(subredTData.shape)\n",
    "#print(subredTData['requester_subreddits_at_request'][1])\n",
    "\n",
    "# create a corpus of subreddits to vectorize\n",
    "trainCorpus = []\n",
    "rTCorpus = []\n",
    "rDCorpus = []\n",
    "\n",
    "for index in range(len(subredTData)):\n",
    "    trainCorpus.append(' '.join(subredTData['requester_subreddits_at_request'][index]))\n",
    "    rTCorpus.append(' '.join(subredTData['requester_subreddits_at_request'][index]))\n",
    "\n",
    "devCorpus = []\n",
    "for index in range(len(subredDData)):\n",
    "    devCorpus.append(' '.join(subredDData['requester_subreddits_at_request'][index]))\n",
    "    rDCorpus.append(' '.join(subredDData['requester_subreddits_at_request'][index]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"FoodstuffsAllAround IAmA RandomActsOfCookies RandomActsofCards RandomKindness Random_Acts_Of_Pizza comiccon cosplay cosplayers So it's been a while since it's happened, but yeah. Just got dumped by a girl I like... My brother has used up pretty much everything in the freezer. I would really appreciate a pizza right now... So it's been a while since it's happened, but yeah. Just got dumped by a girl I like... My brother has used up pretty much everything in the freezer. I would really appreciate a pizza right now... [Request] Just got dumped, no food in the freezer. Pizza?\", \"AskReddit Guitar Jazz Music NSFW_GIF Psychonaut RoomPorn StAugustine TwoXChromosomes WTF YouShouldKnow atheism aww bakedart catpictures cats crochet dubstep ents entwives food funny gonewild hiphopheads listentothis meetup offbeat pics realpics self sex tattoos treecomics treemusic trees videos vinyl zombies My boyfriend and I live in Saint Augustine, Florida and have been having a rough time financially the past few months.  In and out of various jobs, we've had to survive off of coscto sized ramen packs, and pasta and olive oil.  I applied for food stamps a couple days ago, and am waiting to hear back from them.  It's getting a little trite, and we're quite hungry tonight, a hot pizza would be a delight.  We'll happily pay it forward in the future.  Much love. My boyfriend and I live in Saint Augustine, Florida and have been having a rough time financially the past few months.  In and out of various jobs, we've had to survive off of coscto sized ramen packs, and pasta and olive oil.  I applied for food stamps a couple days ago, and am waiting to hear back from them.  It's getting a little trite, and we're quite hungry tonight, a hot pizza would be a delight.  We'll happily pay it forward in the future.  Much love. [Request]  Saint Augustine, US.  Boyfriend and I have no money till next week, and are awaiting food stamps approval.\", \"Albany AskReddit Brooklyn Favors ImGoingToHellForThis Random_Acts_Of_Amazon Random_Acts_Of_Pizza YouShouldKnow cars cigars gaming pics shutupandtakemymoney ualbany upstate_new_york videos I seriously love buffalo chicken pizza. Like, straight up addicted. There's a local pizzeria that delivers and they make the best buffalo chicken pizza I've ever had; however if you dont feel safe with that or have a Giftcard to a chain I could care less, I just want a buffalo chicken pizza soooo bad!  I seriously love buffalo chicken pizza. Like, straight up addicted. There's a local pizzeria that delivers and they make the best buffalo chicken pizza I've ever had; however if you dont feel safe with that or have a Giftcard to a chain I could care less, I just want a buffalo chicken pizza soooo bad!  [Request] I'd love a Buffalo Chicken Puzza!\"]\n",
      "[0, 0, 1]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      " [' Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated Request Colorado Springs Help Us Please', 'AskReddit Eve IAmA MontereyBay RandomKindness RedditBiography dubstep gamecollecting gaming halo i18n techsupport I spent the last money I had on gas today. Im broke until next Thursday :( I spent the last money I had on gas today. Im broke until next Thursday :( [Request] California, No cash and I could use some dinner', \" My girlfriend decided it would be a good idea to get off at Perth bus station when she was coming to visit me and has since had to spend all her money on a taxi to get to me here in Dundee. Any chance some kind soul would get us some pizza since we don't have any cash anymore? My girlfriend decided it would be a good idea to get off at Perth bus station when she was coming to visit me and has since had to spend all her money on a taxi to get to me here in Dundee. Any chance some kind soul would get us some pizza since we don't have any cash anymore? [Request] Hungry couple in Dundee, Scotland would love some pizza!\"]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# combine all text sources into a single corpus\n",
    "fldTText = allTData[['request_id','request_text', 'request_text_edit_aware', 'request_title']]\n",
    "fldDText = allDData[['request_id','request_text', 'request_text_edit_aware', 'request_title']]\n",
    "\n",
    "trainCorpus = []\n",
    "for index in range(len(subredTData)):\n",
    "    a = ' '.join(subredTData['requester_subreddits_at_request'][index])\n",
    "    b = (a, fldTText['request_text'][index], fldTText['request_text_edit_aware'][index],\n",
    "        fldTText['request_title'][index])\n",
    "    trainCorpus.append(' '.join(b))\n",
    "\n",
    "devCorpus = []\n",
    "for index in range(len(subredDData)):\n",
    "    a = ' '.join(subredDData['requester_subreddits_at_request'][index])\n",
    "    b = (a, fldDText['request_text'][index], fldDText['request_text_edit_aware'][index],\n",
    "         fldDText['request_title'][index])\n",
    "    devCorpus.append(' '.join(b))\n",
    "\n",
    "# Print 3 examples  \n",
    "print (trainCorpus[:3])\n",
    "labels = trainLabel.astype(int)\n",
    "labels = list(labels)\n",
    "print(labels[:3])\n",
    "print('-'*75)\n",
    "\n",
    "print ('\\n' , devCorpus[:3])\n",
    "labels_dev = devLabel.astype(int)\n",
    "labels_dev = list(labels_dev)\n",
    "print(labels_dev[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 2. Simple Feature Selection and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Simple Pre-Processing\n",
    "\n",
    "def data_preprocessor(s):\n",
    "    \"\"\"\n",
    "    Note: this function pre-processors data:\n",
    "    (1) removes non-alpha characters\n",
    "    (2) converts digits to 'number'\n",
    "    (3) regularizes spaces (although CountVectorizer ignores this unless they are part of words)\n",
    "    (4) reduces word size to n\n",
    "    \"\"\"\n",
    "\n",
    "    s = [re.sub(r'[?|$|.|!|@|\\n|(|)|<|>|_|-|,|\\']',r' ',s) for s in s] # strip out non-alpha numeric char, replace with space\n",
    "    s = [re.sub(r'\\d+',r'number ',s) for s in s] # convert digits to number\n",
    "    s = [re.sub(r' +',r' ',s) for s in s] # convert multiple spaces to single space\n",
    "    \n",
    "    # This sets word size to n=5\n",
    "    num = 5\n",
    "    def set_word(s):\n",
    "        temp = []\n",
    "        for s in s:\n",
    "            x = s.split()\n",
    "            z = [elem[:num] for elem in x]\n",
    "            z = ' '.join(z)\n",
    "            temp.append(z)       \n",
    "        return temp\n",
    "    \n",
    "    s = set_word(s)\n",
    "    \n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = {'I','about', 'a', 'an', 'are', 'as', 'at', 'be', 'by', 'com', 'for', 'from', 'how',\n",
    "        'in','is','it','of','on','or','that','the','this','to','was','what','when','where',\n",
    "        'who','will', 'with', 'the','www'}\n",
    "stop_long = {\"a\",\"able\",\"about\",\"above\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\n",
    "        \"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"after\",\"afterwards\",\n",
    "        \"again\",\"against\",\"ah\",\"all\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\n",
    "        \"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"announce\",\"another\",\"any\",\"anybody\",\n",
    "        \"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\n",
    "        \"approximately\",\"are\",\"aren\",\"arent\",\"arise\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\n",
    "        \"at\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"be\",\"became\",\"because\",\"become\",\n",
    "        \"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\n",
    "        \"begins\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"between\",\"beyond\",\n",
    "        \"biol\",\"both\",\"brief\",\"briefly\",\"but\",\"by\",\"c\",\"ca\",\"came\",\"can\",\"cannot\",\"can't\",\n",
    "        \"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\n",
    "        \"contains\",\"could\",\"couldnt\",\"d\",\"date\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\n",
    "        \"doesn't\",\"doing\",\"done\",\"don't\",\"down\",\"downwards\",\"due\",\"during\",\"e\",\"each\",\"ed\",\n",
    "        \"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\n",
    "        \"enough\",\"especially\",\"et\",\"et-al\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\n",
    "        \"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"few\",\"ff\",\"fifth\",\"first\",\"five\",\n",
    "        \"fix\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\n",
    "        \"from\",\"further\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\n",
    "        \"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"had\",\"happens\",\"hardly\",\"has\",\"hasn't\",\n",
    "        \"have\",\"haven't\",\"having\",\"he\",\"hed\",\"hence\",\"her\",\"here\",\"hereafter\",\"hereby\",\"herein\",\n",
    "        \"heres\",\"hereupon\",\"hers\",\"herself\",\"hes\",\"hi\",\"hid\",\"him\",\"himself\",\"his\",\"hither\",\n",
    "        \"home\",\"how\",\"howbeit\",\"however\",\"hundred\",\"i\",\"id\",\"ie\",\"if\",\"i'll\",\"im\",\"immediate\",\n",
    "        \"immediately\",\"importance\",\"important\",\"in\",\"inc\",\"indeed\",\"index\",\"information\",\n",
    "        \"instead\",\"into\",\"invention\",\"inward\",\"is\",\"isn't\",\"it\",\"itd\",\"it'll\",\"its\",\"itself\",\n",
    "        \"i've\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\n",
    "        \"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\n",
    "        \"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\n",
    "        \"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"means\",\"meantime\",\n",
    "        \"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"more\",\"moreover\",\"most\",\n",
    "        \"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"my\",\"myself\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\n",
    "        \"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\n",
    "        \"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"no\",\"nobody\",\"non\",\"none\",\"nonetheless\",\n",
    "        \"noone\",\"nor\",\"normally\",\"nos\",\"not\",\"noted\",\"nothing\",\"now\",\"nowhere\",\"o\",\"obtain\",\n",
    "        \"obtained\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"on\",\"once\",\n",
    "        \"one\",\"ones\",\"only\",\"onto\",\"or\",\"ord\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\n",
    "        \"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"owing\",\"own\",\"p\",\"page\",\"pages\",\n",
    "        \"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\n",
    "        \"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\n",
    "        \"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\n",
    "        \"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"re\",\"readily\",\"really\",\"recent\",\n",
    "        \"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\n",
    "        \"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"s\",\"said\",\n",
    "        \"same\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\n",
    "        \"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"she\",\"shed\",\n",
    "        \"she'll\",\"shes\",\"should\",\"shouldn't\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\n",
    "        \"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"so\",\n",
    "        \"some\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\n",
    "        \"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\n",
    "        \"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"such\",\n",
    "        \"sufficiently\",\"suggest\",\"sup\",\"sure\",\"t\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\n",
    "        \"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that'll\",\"thats\",\"that've\",\"the\",\"their\",\n",
    "        \"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"thereafter\",\"thereby\",\"thered\",\n",
    "        \"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\n",
    "        \"there've\",\"these\",\"they\",\"theyd\",\"they'll\",\"theyre\",\"they've\",\"think\",\"this\",\"those\",\n",
    "        \"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"through\",\"throughout\",\"thru\",\"thus\",\n",
    "        \"til\",\"tip\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\n",
    "        \"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlike\",\n",
    "        \"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\n",
    "        \"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"ve\",\"very\",\"via\",\"viz\",\n",
    "        \"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasnt\",\"way\",\"we\",\"wed\",\"welcome\",\"we'll\",\n",
    "        \"went\",\"were\",\"werent\",\"we've\",\"what\",\"whatever\",\"what'll\",\"whats\",\"when\",\"whence\",\n",
    "        \"whenever\",\"where\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\n",
    "        \"wherever\",\"whether\",\"which\",\"while\",\"whim\",\"whither\",\"who\",\"whod\",\"whoever\",\"whole\",\n",
    "        \"who'll\",\"whom\",\"whomever\",\"whos\",\"whose\",\"why\",\"widely\",\"willing\",\"wish\",\"with\",\"within\",\n",
    "        \"without\",\"wont\",\"words\",\"world\",\"would\",\"wouldnt\",\"www\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\n",
    "        \"youd\",\"you'll\",\"your\",\"youre\",\"yours\",\"yourself\",\"yourselves\",\"you've\",\"z\",\"zero\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data:\n",
      "The size of the vocabulary for the training text data is 17213\n",
      "First 5 feature Names: ['000', '0000', '0011011001111000', '00243364', '00pm'] \n",
      "\n",
      "\n",
      "Pre-Processed data:\n",
      "The size of the vocabulary for the training text data is 10491\n",
      "First 5 feature Names: ['aaaaa', 'aan', 'ab', 'aback', 'aband'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the data with CountVectorizer\n",
    "\n",
    "#vectorizer = CountVectorizer(lowercase=True, strip_accents='unicode',stop_words='english')\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1,lowercase=True)\n",
    "tVector = vectorizer.fit_transform(trainCorpus)\n",
    "dVector = vectorizer.transform(devCorpus)\n",
    "\n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", tVector.shape[1])\n",
    "print (\"First 5 feature Names:\", vectorizer.get_feature_names()[1:6], \"\\n\")\n",
    "\n",
    "tVector_p = vectorizer.fit_transform(data_preprocessor(trainCorpus))\n",
    "dVector_p = vectorizer.transform(data_preprocessor(devCorpus))\n",
    "\n",
    "print ('\\nPre-Processed data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", tVector_p.shape[1])\n",
    "print (\"First 5 feature Names:\", vectorizer.get_feature_names()[1:6], \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data:\n",
      "The size of the vocabulary for the training text data is 122674\n",
      "First 5 feature Names: ['0 00', '0 00243364', '0 012468', '0 02', '0 024856'] \n",
      "\n",
      "\n",
      "Raw data:\n",
      "The size of the vocabulary for the training text data is 113396\n",
      "First 5 feature Names: ['a activ', 'a airso', 'a am', 'a amp', 'a anyth'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it with bigrams \n",
    "biVectorizer = CountVectorizer(analyzer = 'word', ngram_range=(2, 2), \n",
    "                               token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "biTrainVector = biVectorizer.fit_transform(trainCorpus)\n",
    "biDVector = biVectorizer.transform(data_preprocessor(devCorpus))\n",
    "\n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", biTrainVector.shape[1])\n",
    "print (\"First 5 feature Names:\", biVectorizer.get_feature_names()[1:6], \"\\n\")\n",
    "\n",
    "biVectorizer_p = CountVectorizer(analyzer = 'word', ngram_range=(2, 2), \n",
    "                                 token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "biTrainVector_p = biVectorizer_p.fit_transform(data_preprocessor(trainCorpus))\n",
    "biDVector_p = biVectorizer_p.transform(data_preprocessor(devCorpus))\n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", biTrainVector_p.shape[1])\n",
    "print (\"First 5 feature Names:\", biVectorizer_p.get_feature_names()[1:6], \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data:\n",
      "The size of the vocabulary for the training text data is 218794\n",
      "First 5 feature Names: ['0 00 even', '0 00 left', '0 00243364 bitcoins', '0 012468 0', '0 02 bank'] \n",
      "\n",
      "\n",
      "Raw data:\n",
      "The size of the vocabulary for the training text data is 116681\n",
      "First 5 feature Names: ['aa meeti', 'aaaaa actua', 'aaaaa adven', 'aaaaa agnos', 'aaaaa ainbo'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it with trigrams \n",
    "\n",
    "triVectorizer = CountVectorizer(analyzer = 'word', ngram_range=(3, 3), \n",
    "                                token_pattern=r'\\b\\w+\\b', min_df=1,stop_words = stop)\n",
    "triTrainVector = triVectorizer.fit_transform(trainCorpus)\n",
    "triDVector = triVectorizer.transform(data_preprocessor(devCorpus))\n",
    "\n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", triTrainVector.shape[1])\n",
    "print (\"First 5 feature Names:\", triVectorizer.get_feature_names()[1:6], \"\\n\")\n",
    "\n",
    "triVectorizer_p = CountVectorizer(analyzer = 'word', ngram_range=(2, 2), \n",
    "                                  token_pattern=r'\\b\\w+\\b', min_df=1,stop_words = stop)\n",
    "triTrainVector_p = triVectorizer_p.fit_transform(data_preprocessor(trainCorpus))\n",
    "triDVector_p = triVectorizer_p.transform(data_preprocessor(devCorpus))\n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", triTrainVector_p.shape[1])\n",
    "print (\"First 5 feature Names:\", triVectorizer_p.get_feature_names()[1:6], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features = 206106\n",
      "\n",
      "Raw data:\n",
      "The size of the vocabulary for the training text data is 206106\n",
      "First 5 feature Names: ['aband achie alien', 'aband achie alien askre', 'aband achie alien askre bizar', 'aband achie alien askre bizar bugwa', 'aband advan', 'aband advan advic', 'aband advan advic askre', 'aband advan advic askre diabl', 'aband advan advic askre diabl guild', 'aband advic', 'aband advic advic', 'aband advic advic arche', 'aband advic advic arche askan', 'aband advic advic arche askan askco', 'aband advic advic askre', 'aband advic advic askre bgsu', 'aband advic advic askre bgsu bunde', 'aband advic album', 'aband advic album andro', 'aband advic album andro andro', 'aband advic album andro andro askre', 'aband advic album antij', 'aband advic album antij anyth', 'aband advic album antij anyth askre', 'aband advic andro', 'aband advic andro askre', 'aband advic andro askre autod', 'aband advic andro askre autod autos', 'aband advic anima'] \n",
      "\n",
      "Number of features = 4471\n",
      "\n",
      "Raw data:\n",
      "The size of the vocabulary for the training text data is 4471\n",
      "First 5 feature Names: ['abdl', 'abero', 'ablet', 'abrat', 'acade', 'acapp', 'accid', 'accom', 'accou', 'acdef', 'aceof', 'acgay', 'achie', 'acne', 'acous', 'acrop', 'actin', 'actio', 'actra', 'acts', 'actso', 'actua', 'actur', 'adamc', 'add', 'adder', 'adela', 'adhd', 'adhdm'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tf-idf vectorizer\n",
    "stop = {'I','about', 'a', 'aa', 'aaa','aaaa','aaaaa','ab','aan','an', 'are', 'as', 'at', 'be', 'by', 'com', 'for', 'from', 'how',\n",
    "        'in','is','it','of','on','or','that','the','this','to','was','what','when','where',\n",
    "        'who','will', 'with', 'the','www'}\n",
    "vectorizer = TfidfVectorizer(min_df=1,ngram_range=(2, 6),stop_words = stop)\n",
    "trainVector = vectorizer.fit_transform(data_preprocessor(trainCorpus))\n",
    "numF = trainVector.shape[1]\n",
    "print(\"Number of features =\", numF)\n",
    "devVector = vectorizer.transform(data_preprocessor(devCorpus))\n",
    "\n",
    "    \n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", devVector.shape[1])\n",
    "print (\"First 5 feature Names:\", vectorizer.get_feature_names()[1:30], \"\\n\")\n",
    "\n",
    "#subreddits\n",
    "Rvectorizer = TfidfVectorizer(min_df=1, stop_words = stop)\n",
    "RtrainVector = Rvectorizer.fit_transform(data_preprocessor(rTCorpus))\n",
    "numF = RtrainVector.shape[1]\n",
    "print(\"Number of features =\", numF)\n",
    "RdevVector = Rvectorizer.transform(data_preprocessor(rDCorpus))\n",
    "    \n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", numF)\n",
    "print (\"First 5 feature Names:\", Rvectorizer.get_feature_names()[1:30], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 3. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C =  0.01 Logistic regression accuracy: 0.736\n",
      "For C =  0.01 Logistic regression (processed data) accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "C = 0.01 #(For now)\n",
    "\n",
    "modelLogit = LogisticRegression(penalty='l2', C=C)\n",
    "modelLogit.fit(tVector,trainLabel)\n",
    "logitScore = round(modelLogit.score(dVector, devLabel), 4)\n",
    "print(\"For C = \", C, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "modelLogit.fit(tVector_p,trainLabel)\n",
    "logitScore = round(modelLogit.score(dVector_p, devLabel), 4)\n",
    "print(\"For C = \", C, \"Logistic regression (processed data) accuracy:\", logitScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C =  0.01 Logistic regression accuracy: 0.742\n",
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.93      0.83       740\n",
      "       True       0.41      0.15      0.22       260\n",
      "\n",
      "avg / total       0.67      0.72      0.67      1000\n",
      "\n",
      "For C =  0.01 Logistic regression accuracy: 0.724\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression: bigrams\n",
    "# fit a logistic regression model to our bigrams, using l2 regularization and C = .5\n",
    "biModelLogit = LogisticRegression(penalty='l2', C=.5)\n",
    "biModelLogit.fit(biTrainVector, trainLabel)\n",
    "    # print out the features with the biggest weights for each category,\n",
    "    # across all categories    \n",
    "logitScore = round(biModelLogit.score(biDVector, devLabel), 4)\n",
    "print(\"For C = \", C, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "biModelLogit_p = LogisticRegression(penalty='l2', C=.5)\n",
    "biModelLogit_p.fit(biTrainVector_p, trainLabel)\n",
    "\n",
    "# print out the classification report\n",
    "print(\"Classification report:\\n\")\n",
    "devPred = biModelLogit_p.predict(biDVector_p)\n",
    "print(classification_report(devLabel, devPred))\n",
    "\n",
    "# print the accuracy score\n",
    "logitScore = round(biModelLogit_p.score(biDVector_p, devLabel), 4)\n",
    "print(\"For C = \", C, \"Logistic regression accuracy:\", logitScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C =  0.01 Logistic regression accuracy: 0.738\n",
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.96      0.85       740\n",
      "       True       0.54      0.12      0.19       260\n",
      "\n",
      "avg / total       0.70      0.74      0.68      1000\n",
      "\n",
      "For C =  0.01 Logistic regression accuracy: 0.744\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression: trigrams\n",
    "# fit a logistic regression model to our bigrams, using l2 regularization and C = .5\n",
    "triModelLogit = LogisticRegression(penalty='l2', C=.5)\n",
    "triModelLogit.fit(triTrainVector, trainLabel)\n",
    "    # print out the features with the biggest weights for each category,\n",
    "    # across all categories    \n",
    "logitScore = round(triModelLogit.score(triDVector, devLabel), 4)\n",
    "print(\"For C = \", C, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "triModelLogit_p = LogisticRegression(penalty='l2', C=.5)\n",
    "triModelLogit_p.fit(triTrainVector_p, trainLabel)\n",
    "\n",
    "# print out the classification report\n",
    "print(\"Classification report:\\n\")\n",
    "devPred = triModelLogit_p.predict(triDVector_p)\n",
    "print(classification_report(devLabel, devPred))\n",
    "    \n",
    "# print the accuracy score\n",
    "logitScore = round(triModelLogit_p.score(triDVector_p, devLabel), 4)\n",
    "print(\"For C = \", C, \"Logistic regression accuracy:\", logitScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.75      0.98      0.85       740\n",
      "       True       0.45      0.06      0.10       260\n",
      "\n",
      "avg / total       0.67      0.74      0.65      1000\n",
      "\n",
      "% Bad Predictions = 0.263 \n",
      "\n",
      "For C =  100 Logistic regression accuracy: 0.737\n"
     ]
    }
   ],
   "source": [
    "#tf-idf # how does it look?\n",
    "reg = [100]\n",
    "for c in reg: \n",
    "    modelLogit = LogisticRegression(penalty='l2', C=c )\n",
    "    modelLogit.fit(trainVector, trainLabel)\n",
    "    devPred = modelLogit.predict(devVector)\n",
    "    # get the f1 score per category \n",
    "    f1 = metrics.f1_score(devLabel, devPred, average = None)\n",
    "    # print the classification report\n",
    "    print(\"Classification report:\\n\")\n",
    "    devPred = modelLogit.predict(devVector)\n",
    "    print(classification_report(devLabel, devPred))\n",
    "\n",
    "    # get the prediction probabilities\n",
    "    devPredP = modelLogit.predict_proba(devVector)\n",
    "\n",
    "    print (\"% Bad Predictions =\", sum(devPred != devLabel)/len(devLabel), '\\n')\n",
    "    logitScore = round(modelLogit.score(devVector, devLabel), 4)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.74      0.86      0.80       740\n",
      "       True       0.29      0.16      0.21       260\n",
      "\n",
      "avg / total       0.63      0.68      0.64      1000\n",
      "\n",
      "SubRedits % Bad Predictions = 0.322 \n",
      "\n",
      "For C =  100 Logistic regression accuracy on SubRedits: 0.678\n"
     ]
    }
   ],
   "source": [
    "#TFIDF results: subredits only\n",
    "\n",
    "reg = [100]\n",
    "for c in reg:     \n",
    "    RmodelLogit = LogisticRegression(penalty='l2', C=c )\n",
    "    RmodelLogit.fit(RtrainVector, trainLabel)\n",
    "    RdevPred = RmodelLogit.predict(RdevVector)\n",
    "    # get the f1 score per category \n",
    "    f1 = metrics.f1_score(devLabel, RdevPred, average = None)\n",
    "    # print the classification report\n",
    "    print(\"Classification report:\\n\")\n",
    "    RdevPred = RmodelLogit.predict(RdevVector)\n",
    "    print(classification_report(devLabel, RdevPred))\n",
    "\n",
    "    # get the prediction probabilities\n",
    "    RdevPredP = RmodelLogit.predict_proba(RdevVector)\n",
    "\n",
    "    print (\"SubRedits % Bad Predictions =\", sum(RdevPred != devLabel)/len(devLabel), '\\n')\n",
    "    logitScore = round(RmodelLogit.score(RdevVector, devLabel), 4)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy on SubRedits:\", logitScore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB using raw data with alpha = 0.010: 0.719\n",
      "Bernoulli NB using processed data  with alpha = 0.010: 0.708\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "clf = BernoulliNB(alpha=alpha)\n",
    "clf.fit(tVector, trainLabel)\n",
    "test_predicted_labels = clf.predict(dVector) \n",
    "print ('Bernoulli NB using raw data with alpha = %1.3f:' %alpha, metrics.accuracy_score(devLabel,test_predicted_labels) )\n",
    "\n",
    "clf.fit(tVector_p, trainLabel)\n",
    "test_predicted_labels = clf.predict(dVector_p) \n",
    "print ('Bernoulli NB using processed data  with alpha = %1.3f:' %alpha, metrics.accuracy_score(devLabel,test_predicted_labels) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic Regression More Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C =  0.0001 Logistic regression accuracy: 0.74\n",
      "For C =  0.001 Logistic regression accuracy: 0.74\n",
      "For C =  0.01 Logistic regression accuracy: 0.74\n",
      "For C =  0.1 Logistic regression accuracy: 0.734\n",
      "For C =  0.5 Logistic regression accuracy: 0.673\n",
      "For C =  1.0 Logistic regression accuracy: 0.652\n",
      "For C =  2.0 Logistic regression accuracy: 0.653\n",
      "For C =  6.0 Logistic regression accuracy: 0.644\n",
      "For C =  10.0 Logistic regression accuracy: 0.646\n",
      "0.0\n",
      "\n",
      "Top 5 Weighted Features:\n",
      "3500\n",
      "gamet 0.0\n",
      "3501\n",
      "gamin 0.0\n",
      "3502\n",
      "gamut 0.0\n",
      "3484\n",
      "galwa 0.0\n",
      "10490\n",
      "zzm 0.0\n",
      "Majority Class Probability: 0.74\n",
      "0     False\n",
      "1     False\n",
      "2     False\n",
      "3     False\n",
      "4     False\n",
      "5      True\n",
      "6     False\n",
      "7     False\n",
      "8     False\n",
      "9      True\n",
      "10     True\n",
      "11    False\n",
      "12    False\n",
      "13    False\n",
      "14    False\n",
      "15    False\n",
      "16     True\n",
      "17    False\n",
      "18     True\n",
      "19    False\n",
      "Name: requester_received_pizza, dtype: bool\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# get the best regularization\n",
    "regStrength = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 6.0, 10.0]\n",
    "\n",
    "\n",
    "for c in regStrength:\n",
    "    modelLogit = LogisticRegression(penalty='l1', C=c)\n",
    "    modelLogit.fit(tVector_p, trainLabel)\n",
    "    logitScore = round(modelLogit.score(dVector_p, devLabel), 4)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "# although the best score comes from c=.001, the bet F1-score \n",
    "# comes from c=.5, and this gives better weight options\n",
    "modelLogit = LogisticRegression(penalty='l1', C=.001, tol = .1)\n",
    "modelLogit.fit(tVector_p, trainLabel)\n",
    "\n",
    "print(max(modelLogit.coef_[0]))\n",
    "numWeights = 5\n",
    "\n",
    "sortIndex = np.argsort(modelLogit.coef_)\n",
    "iLen = len(sortIndex[0])\n",
    "print(\"\\nTop\", numWeights, \"Weighted Features:\")\n",
    "\n",
    "for index in range((iLen - numWeights) , iLen):\n",
    "    lookup = sortIndex[0][index]\n",
    "    print(lookup)\n",
    "    weight =  modelLogit.coef_[0][lookup]\n",
    "    print(vectorizer.get_feature_names()[sortIndex[0][index]], weight)\n",
    "    \n",
    "print(\"Majority Class Probability:\", 1 - sum(devLabel)/len(devLabel))\n",
    "print(devLabel[:20])\n",
    "print(modelLogit.predict(dVector_p)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* More data pre-processing (looking for newer features too)\n",
    "* Explore PCA/LSA\n",
    "* Ideas on features\n",
    "    - Combination of words\n",
    "    - Pruning\n",
    "    - Timing (of requests)\n",
    "    - Location\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
