{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import Libraries ##\n",
    "from nltk.stem import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pandas import *\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the normalized Data: (3040, 32)\n"
     ]
    }
   ],
   "source": [
    "## Get Data ##\n",
    "\n",
    "#reference on data: https://www.kaggle.com/c/random-acts-of-pizza/data\n",
    "# pull in the training and test data\n",
    "with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/train.json', encoding='utf-8') as data_file:\n",
    "    trainData = json.loads(data_file.read())   \n",
    "with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/test.json', encoding='utf-8') as data_file:\n",
    "    testData = json.loads(data_file.read())    \n",
    "\n",
    "# create a dev data set \n",
    "devData = trainData[0:1000]\n",
    "trainData = trainData[1000:]\n",
    "\n",
    "# show how the data looks in its original format\n",
    "#pprint(\"data in json format:\")\n",
    "#pprint(trainData[1])\n",
    "\n",
    "# create a normalized view\n",
    "allTrainData = json_normalize(trainData)\n",
    "print(\"\\nSize of the normalized Data:\", allTrainData.shape)\n",
    "#print(\"\\nnormalized data columns:\", list(allTData))\n",
    "\n",
    "allDevData = json_normalize(devData)\n",
    "allTestData = json_normalize(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3040, 1)\n"
     ]
    }
   ],
   "source": [
    "## Create subsets of data for analysis ###\n",
    "\n",
    "# create a flat dataset without the subreddits list\n",
    "flatData = allTrainData.drop('requester_subreddits_at_request', 1)\n",
    "# create a separate dataset with just subreddits, indexed on request id\n",
    "# we can creata a count vector on the words, run Naive Bayes against it, \n",
    "# and add the probabilities to our flat dataset\n",
    "subredTData = allTrainData[['request_id','requester_subreddits_at_request']]\n",
    "subredTData.set_index('request_id', inplace=True)\n",
    "\n",
    "subredDData= allDevData[['request_id','requester_subreddits_at_request']]\n",
    "subredDData.set_index('request_id', inplace=True)\n",
    "\n",
    "# our training labels\n",
    "trainLabel = allTrainData['requester_received_pizza']\n",
    "\n",
    "devLabel = allDevData['requester_received_pizza']\n",
    "\n",
    "# what do these look like?\n",
    "print(subredTData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Clean Data ##\n",
    "def cleanData(dataSet, trimInd, tokenInd):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    returnData = []\n",
    "        \n",
    "    for line in dataSet:\n",
    "        # if the line is already tokenized, stem the tokens, and join the line \n",
    "        # to continue data cleanup\n",
    "        if tokenInd:\n",
    "            line = ([stemmer.stem(word) for word in line])\n",
    "            line = ' '.join(line)\n",
    "        else:\n",
    "        #tokenize so nltk stemmer can stem each word\n",
    "            tokens = \"\".join([i for i in line]).split(\" \")\n",
    "            # stem and return string format\n",
    "            line = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "        #lower case\n",
    "        line = line.lower()\n",
    "        # replace digits with the number 1\n",
    "        line = re.sub('\\d+', '1', line)\n",
    "        #remove non alphanumeric text\n",
    "        line = re.sub('\\W', ' ', line)\n",
    "        # remove underscores\n",
    "        line = re.sub('_', '', line)\n",
    "        # truncate words longer than 15 characters\n",
    "        if trimInd:\n",
    "            regex_long_words = re.compile(r\"(\\w{1,15})\\w*\")   \n",
    "            listMatches = re.findall(regex_long_words, line)\n",
    "            line = ' '.join(listMatches)\n",
    "        \n",
    "        \n",
    "        returnData.append(line)\n",
    "\n",
    "    return returnData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean All the Text\n",
    "pandas.options.mode.chained_assignment = None  # default='warn'\n",
    "# clean data\n",
    "cleanSubRedCol = cleanData(subredTData['requester_subreddits_at_request'], trimInd = False, tokenInd = True)\n",
    "cleanTrainSubRed = subredTData\n",
    "cleanTrainSubRed['requester_subreddits_at_request'] = cleanSubRedCol\n",
    "\n",
    "cleanTrainReqText = cleanData(allTrainData['request_text'], trimInd = True, tokenInd = False)\n",
    "cleanTrainReqTextEdit = cleanData(allTrainData['request_text_edit_aware'], trimInd = True, tokenInd = False)\n",
    "cleanTrainReqTitle = cleanData(allTrainData['request_title'], trimInd = False, tokenInd = False)\n",
    "\n",
    "allCleanTrainData = allTrainData[['request_id', 'request_text', 'request_text_edit_aware', 'request_title']]\n",
    "allCleanTrainData['request_text'] = cleanTrainReqText\n",
    "allCleanTrainData['request_text_edit_aware'] = cleanTrainReqTextEdit                                                                            \n",
    "allCleanTrainData['request_title'] = cleanTrainReqTitle \n",
    "\n",
    "cleanSubRedCol = cleanData(subredDData['requester_subreddits_at_request'], trimInd = False, tokenInd = True)\n",
    "cleanDevSubRed = subredDData\n",
    "cleanDevSubRed['requester_subreddits_at_request'] = cleanSubRedCol\n",
    "\n",
    "cleanDevReqText = cleanData(allDevData['request_text'], trimInd = True, tokenInd = False)\n",
    "cleanDevReqTextEdit = cleanData(allDevData['request_text_edit_aware'], trimInd = True, tokenInd = False)\n",
    "cleanDevReqTitle = cleanData(allDevData['request_title'], trimInd = False, tokenInd = False)\n",
    "\n",
    "allCleanDevData = allDevData[['request_id', 'request_text', 'request_text_edit_aware', 'request_title']]\n",
    "allCleanDevData['request_text'] = cleanDevReqText\n",
    "allCleanDevData['request_text_edit_aware'] = cleanDevReqTextEdit                                                                            \n",
    "allCleanDevData['request_title'] = cleanDevReqTitle \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The size of the vocabulary for the training text data is 10374\n",
      "\n",
      "First 5 feature Names: ['11111n', '11e1cfb', '1a', '1a1', '1a1a1fcafd1'] \n",
      "\n",
      "For C =  0.0001 Logistic regression accuracy: 0.74\n",
      "For C =  0.001 Logistic regression accuracy: 0.74\n",
      "For C =  0.01 Logistic regression accuracy: 0.74\n",
      "For C =  0.1 Logistic regression accuracy: 0.739\n",
      "For C =  0.5 Logistic regression accuracy: 0.69\n",
      "For C =  1.0 Logistic regression accuracy: 0.669\n",
      "For C =  2.0 Logistic regression accuracy: 0.661\n",
      "For C =  6.0 Logistic regression accuracy: 0.649\n",
      "For C =  10.0 Logistic regression accuracy: 0.639\n",
      "0.0\n",
      "\n",
      "Top 25 Weighted Features:\n",
      "followings 0.0\n",
      "follows 0.0\n",
      "fold 0.0\n",
      "fluid 0.0\n",
      "fluently 0.0\n",
      "flu 0.0\n",
      "flip 0.0\n",
      "flippin 0.0\n",
      "flkmm 0.0\n",
      "flma 0.0\n",
      "float 0.0\n",
      "floating 0.0\n",
      "flood 0.0\n",
      "flooded 0.0\n",
      "floor 0.0\n",
      "floored 0.0\n",
      "florida 0.0\n",
      "floridian 0.0\n",
      "flour 0.0\n",
      "flourish 0.0\n",
      "flow 0.0\n",
      "flowing 0.0\n",
      "floxacin 0.0\n",
      "flight 0.0\n",
      "ಠಠ 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainCorpus = []\n",
    "for index in range(len(cleanTrainSubRed)):\n",
    "    a = ' '.join(cleanTrainSubRed['requester_subreddits_at_request'][index])\n",
    "    b = (a, allCleanTrainData['request_text'][index], allCleanTrainData['request_text_edit_aware'][index],\n",
    "         allCleanTrainData['request_title'][index])\n",
    "    trainCorpus.append(' '.join(b))\n",
    "\n",
    "devCorpus = []\n",
    "for index in range(len(subredDData)):\n",
    "    a = ' '.join(cleanDevSubRed['requester_subreddits_at_request'][index])\n",
    "    b = (a, allCleanDevData['request_text'][index], allCleanDevData['request_text_edit_aware'][index],\n",
    "         allCleanDevData['request_title'][index])\n",
    "    devCorpus.append(' '.join(b))\n",
    "    \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "tVector = vectorizer.fit_transform(trainCorpus)\n",
    "dVector = vectorizer.transform(devCorpus)\n",
    "\n",
    "print (\"\\nThe size of the vocabulary for the training text data is\", tVector.shape[1])\n",
    "print(\"\\nFirst 5 feature Names:\", vectorizer.get_feature_names()[1:6], \"\\n\")\n",
    "\n",
    "# get the best regularization\n",
    "regStrength = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 6.0, 10.0]\n",
    "catCoef = []\n",
    "\n",
    "for c in regStrength:\n",
    "    modelLogit = LogisticRegression(penalty='l1', C=c)\n",
    "    modelLogit.fit(tVector, trainLabel)\n",
    "    logitScore = round(modelLogit.score(dVector, devLabel), 4)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "modelLogit = LogisticRegression(penalty='l1', C=.001)\n",
    "modelLogit.fit(tVector, trainLabel)\n",
    "\n",
    "print(max(modelLogit.coef_[0]))\n",
    "numWeights = 25\n",
    "\n",
    "sortIndex = np.argsort(modelLogit.coef_)\n",
    "iLen = len(sortIndex[0])\n",
    "print(\"\\nTop\", numWeights, \"Weighted Features:\")\n",
    "\n",
    "for index in range((iLen - numWeights) , iLen):\n",
    "    lookup = sortIndex[0][index]\n",
    "    weight =  round(modelLogit.coef_[0][lookup], 5)\n",
    "    print(vectorizer.get_feature_names()[sortIndex[0][index]], weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
