{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Import Libraries ##\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pandas import *\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data in json format:'\n",
      "{'giver_username_if_known': 'N/A',\n",
      " 'number_of_downvotes_of_request_at_retrieval': 2,\n",
      " 'number_of_upvotes_of_request_at_retrieval': 2,\n",
      " 'post_was_edited': False,\n",
      " 'request_id': 't3_yemx8',\n",
      " 'request_number_of_comments_at_retrieval': 1,\n",
      " 'request_text': 'My boyfriend and I live in Saint Augustine, Florida and have '\n",
      "                 'been having a rough time financially the past few months.  '\n",
      "                 \"In and out of various jobs, we've had to survive off of \"\n",
      "                 'coscto sized ramen packs, and pasta and olive oil.  I '\n",
      "                 'applied for food stamps a couple days ago, and am waiting to '\n",
      "                 \"hear back from them.  It's getting a little trite, and we're \"\n",
      "                 \"quite hungry tonight, a hot pizza would be a delight.  We'll \"\n",
      "                 'happily pay it forward in the future.  Much love.',\n",
      " 'request_text_edit_aware': 'My boyfriend and I live in Saint Augustine, '\n",
      "                            'Florida and have been having a rough time '\n",
      "                            'financially the past few months.  In and out of '\n",
      "                            \"various jobs, we've had to survive off of coscto \"\n",
      "                            'sized ramen packs, and pasta and olive oil.  I '\n",
      "                            'applied for food stamps a couple days ago, and am '\n",
      "                            \"waiting to hear back from them.  It's getting a \"\n",
      "                            \"little trite, and we're quite hungry tonight, a \"\n",
      "                            \"hot pizza would be a delight.  We'll happily pay \"\n",
      "                            'it forward in the future.  Much love.',\n",
      " 'request_title': '[Request]  Saint Augustine, US.  Boyfriend and I have no '\n",
      "                  'money till next week, and are awaiting food stamps '\n",
      "                  'approval.',\n",
      " 'requester_account_age_in_days_at_request': 444.6098148148148,\n",
      " 'requester_account_age_in_days_at_retrieval': 919.923113425926,\n",
      " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
      " 'requester_days_since_first_post_on_raop_at_retrieval': 475.27163194444444,\n",
      " 'requester_number_of_comments_at_request': 99,\n",
      " 'requester_number_of_comments_at_retrieval': 182,\n",
      " 'requester_number_of_comments_in_raop_at_request': 0,\n",
      " 'requester_number_of_comments_in_raop_at_retrieval': 10,\n",
      " 'requester_number_of_posts_at_request': 6,\n",
      " 'requester_number_of_posts_at_retrieval': 16,\n",
      " 'requester_number_of_posts_on_raop_at_request': 0,\n",
      " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
      " 'requester_number_of_subreddits_at_request': 38,\n",
      " 'requester_received_pizza': False,\n",
      " 'requester_subreddits_at_request': ['AskReddit',\n",
      "                                     'Guitar',\n",
      "                                     'Jazz',\n",
      "                                     'Music',\n",
      "                                     'NSFW_GIF',\n",
      "                                     'Psychonaut',\n",
      "                                     'RoomPorn',\n",
      "                                     'StAugustine',\n",
      "                                     'TwoXChromosomes',\n",
      "                                     'WTF',\n",
      "                                     'YouShouldKnow',\n",
      "                                     'atheism',\n",
      "                                     'aww',\n",
      "                                     'bakedart',\n",
      "                                     'catpictures',\n",
      "                                     'cats',\n",
      "                                     'crochet',\n",
      "                                     'dubstep',\n",
      "                                     'ents',\n",
      "                                     'entwives',\n",
      "                                     'food',\n",
      "                                     'funny',\n",
      "                                     'gonewild',\n",
      "                                     'hiphopheads',\n",
      "                                     'listentothis',\n",
      "                                     'meetup',\n",
      "                                     'offbeat',\n",
      "                                     'pics',\n",
      "                                     'realpics',\n",
      "                                     'self',\n",
      "                                     'sex',\n",
      "                                     'tattoos',\n",
      "                                     'treecomics',\n",
      "                                     'treemusic',\n",
      "                                     'trees',\n",
      "                                     'videos',\n",
      "                                     'vinyl',\n",
      "                                     'zombies'],\n",
      " 'requester_upvotes_minus_downvotes_at_request': 278,\n",
      " 'requester_upvotes_minus_downvotes_at_retrieval': 664,\n",
      " 'requester_upvotes_plus_downvotes_at_request': 378,\n",
      " 'requester_upvotes_plus_downvotes_at_retrieval': 942,\n",
      " 'requester_user_flair': None,\n",
      " 'requester_username': 'Faroffhighways',\n",
      " 'unix_timestamp_of_request': 1345254263.0,\n",
      " 'unix_timestamp_of_request_utc': 1345250663.0}\n",
      "\n",
      "Size of the normalized Data: (3040, 32)\n",
      "\n",
      "normalized data columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "## Get Data ##\n",
    "\n",
    "#reference on data: https://www.kaggle.com/c/random-acts-of-pizza/data\n",
    "# pull in the training and test data\n",
    "with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/train.json', encoding='utf-8') as data_file:\n",
    "    trainData = json.loads(data_file.read())   \n",
    "with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/test.json', encoding='utf-8') as data_file:\n",
    "    testData = json.loads(data_file.read())    \n",
    "\n",
    "# create a dev data set \n",
    "devData = trainData[0:1000]\n",
    "trainData = trainData[1000:]\n",
    "\n",
    "# show how the data looks in its original format\n",
    "pprint(\"data in json format:\")\n",
    "pprint(trainData[1])\n",
    "\n",
    "# create a normalized view\n",
    "allTData = json_normalize(trainData)\n",
    "print(\"\\nSize of the normalized Data:\", allTData.shape)\n",
    "print(\"\\nnormalized data columns:\", list(allTData))\n",
    "\n",
    "allDData = json_normalize(devData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "(3040, 1)\n",
      "['AskReddit', 'Guitar', 'Jazz', 'Music', 'NSFW_GIF', 'Psychonaut', 'RoomPorn', 'StAugustine', 'TwoXChromosomes', 'WTF', 'YouShouldKnow', 'atheism', 'aww', 'bakedart', 'catpictures', 'cats', 'crochet', 'dubstep', 'ents', 'entwives', 'food', 'funny', 'gonewild', 'hiphopheads', 'listentothis', 'meetup', 'offbeat', 'pics', 'realpics', 'self', 'sex', 'tattoos', 'treecomics', 'treemusic', 'trees', 'videos', 'vinyl', 'zombies']\n"
     ]
    }
   ],
   "source": [
    "## Create subsets of data for analysis ###\n",
    "\n",
    "# create a flat dataset without the subreddits list\n",
    "flatData = allTData.drop('requester_subreddits_at_request', 1)\n",
    "# create a separate dataset with just subreddits, indexed on request id\n",
    "# we can creata a count vector on the words, run Naive Bayes against it, \n",
    "# and add the probabilities to our flat dataset\n",
    "subredTData = allTData[['request_id','requester_subreddits_at_request']]\n",
    "subredTData.set_index('request_id', inplace=True)\n",
    "\n",
    "subredDData= allDData[['request_id','requester_subreddits_at_request']]\n",
    "subredDData.set_index('request_id', inplace=True)\n",
    "\n",
    "# our training labels\n",
    "trainLabel = allTData['requester_received_pizza']\n",
    "\n",
    "devLabel = allDData['requester_received_pizza']\n",
    "\n",
    "# what do these look like?\n",
    "print(list(flatData))\n",
    "print(subredTData.shape)\n",
    "print(subredTData['requester_subreddits_at_request'][1])\n",
    "\n",
    "# create a corpus of subreddits to vectorize\n",
    "trainCorpus = []\n",
    "for index in range(len(subredTData)):\n",
    "    trainCorpus.append(' '.join(subredTData['requester_subreddits_at_request'][index]))\n",
    "    \n",
    "devCorpus = []\n",
    "for index in range(len(subredDData)):\n",
    "    devCorpus.append(' '.join(subredDData['requester_subreddits_at_request'][index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The size of the vocabulary for the training data is 6450\n",
      "\n",
      "First 5 feature Names: ['1000words', '100movies365days', '100pushups', '100sets', '1558']\n",
      "For C =  0.0001 Logistic regression accuracy: 0.74\n",
      "For C =  0.001 Logistic regression accuracy: 0.74\n",
      "For C =  0.01 Logistic regression accuracy: 0.74\n",
      "For C =  0.1 Logistic regression accuracy: 0.753\n",
      "For C =  0.5 Logistic regression accuracy: 0.742\n",
      "For C =  1.0 Logistic regression accuracy: 0.734\n",
      "For C =  2.0 Logistic regression accuracy: 0.727\n",
      "For C =  6.0 Logistic regression accuracy: 0.715\n",
      "For C =  10.0 Logistic regression accuracy: 0.704\n",
      "\n",
      "Top 5 Weighted Features:\n",
      "sopa 0.325\n",
      "nfl 0.325\n",
      "bioshock 0.339\n",
      "offbeat 0.347\n",
      "assistance 0.407\n"
     ]
    }
   ],
   "source": [
    "## Analyze ##\n",
    "\n",
    "# create a train vector\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "trainVector = vectorizer.fit_transform(trainCorpus)\n",
    "devVector = vectorizer.transform(devCorpus)\n",
    "print (\"\\nThe size of the vocabulary for the training data is\", trainVector.shape[1])\n",
    "print(\"\\nFirst 5 feature Names:\", vectorizer.get_feature_names()[1:6])\n",
    "\n",
    "# get the best regularization\n",
    "regStrength = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 6.0, 10.0]\n",
    "catCoef = []\n",
    "\n",
    "for c in regStrength:\n",
    "    modelLogit = LogisticRegression(penalty='l2', C=c)\n",
    "    modelLogit.fit(trainVector, trainLabel)\n",
    "    logitScore = round(modelLogit.score(devVector, devLabel), 3)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "\n",
    "\n",
    "numWeights = 5\n",
    "\n",
    "\n",
    "# Fit a logistic regression on training data\n",
    "modelLogit = LogisticRegression(penalty='l2', C=.1)\n",
    "\n",
    "modelLogit.fit(trainVector, trainLabel)\n",
    "\n",
    "indexArray = np.zeros(numWeights)\n",
    "sortIndex = np.argsort(modelLogit.coef_)\n",
    "iLen = len(sortIndex[0])\n",
    "print(\"\\nTop 5 Weighted Features:\")\n",
    "\n",
    "for index in range((iLen - numWeights) , iLen):\n",
    "    lookup = sortIndex[0][index]\n",
    "    weight =  round(modelLogit.coef_[0][lookup], 3)\n",
    "    print(vectorizer.get_feature_names()[sortIndex[0][index]], weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The size of the vocabulary for the training text data is 10833\n",
      "\n",
      "First 5 feature Names: ['000', '0000', '0011011001111000', '00pm', '012468']\n",
      "For C =  0.0001 Logistic regression accuracy: 0.74\n",
      "For C =  0.001 Logistic regression accuracy: 0.74\n",
      "For C =  0.01 Logistic regression accuracy: 0.74\n",
      "For C =  0.1 Logistic regression accuracy: 0.722\n",
      "For C =  0.5 Logistic regression accuracy: 0.698\n",
      "For C =  1.0 Logistic regression accuracy: 0.694\n",
      "For C =  2.0 Logistic regression accuracy: 0.689\n",
      "For C =  6.0 Logistic regression accuracy: 0.675\n",
      "For C =  10.0 Logistic regression accuracy: 0.666\n",
      "\n",
      "Top  25 Weighted Features:\n",
      "paying 0.001\n",
      "check 0.001\n",
      "thursday 0.001\n",
      "http 0.001\n",
      "went 0.001\n",
      "ve 0.001\n",
      "thank 0.001\n",
      "he 0.001\n",
      "tough 0.001\n",
      "kindness 0.001\n",
      "back 0.001\n",
      "exchange 0.001\n",
      "bucks 0.001\n",
      "paycheck 0.001\n",
      "sunday 0.001\n",
      "com 0.001\n",
      "rice 0.001\n",
      "bills 0.001\n",
      "request 0.001\n",
      "wife 0.001\n",
      "her 0.001\n",
      "imgur 0.001\n",
      "jpg 0.002\n",
      "edit 0.002\n",
      "she 0.002\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text\n",
    "reqTText = allTData[['request_id','request_text']]\n",
    "reqTText.set_index('request_id', inplace=True)\n",
    "#print(reqTText[:5])\n",
    "reqDText = allDData[['request_id','request_text']]\n",
    "reqDText.set_index('request_id', inplace=True)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "reqTVector = vectorizer.fit_transform(reqTText['request_text'])\n",
    "reqDVector = vectorizer.transform(reqDText['request_text'])\n",
    "\n",
    "print (\"\\nThe size of the vocabulary for the training text data is\", reqTVector.shape[1])\n",
    "print(\"\\nFirst 5 feature Names:\", vectorizer.get_feature_names()[1:6])\n",
    "\n",
    "# get the best regularization\n",
    "regStrength = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 6.0, 10.0]\n",
    "catCoef = []\n",
    "\n",
    "for c in regStrength:\n",
    "    modelLogit = LogisticRegression(penalty='l2', C=c)\n",
    "    modelLogit.fit(reqTVector, trainLabel)\n",
    "    logitScore = round(modelLogit.score(reqDVector, devLabel), 4)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "numWeights = 25\n",
    "\n",
    "\n",
    "# Fit a logistic regression on training data\n",
    "modelLogit = LogisticRegression(penalty='l2', C=.0001)\n",
    "\n",
    "modelLogit.fit(reqTVector, trainLabel)\n",
    "\n",
    "indexArray = np.zeros(numWeights)\n",
    "sortIndex = np.argsort(modelLogit.coef_)\n",
    "iLen = len(sortIndex[0])\n",
    "print(\"\\nTop\", numWeights, \"Weighted Features:\")\n",
    "\n",
    "for index in range((iLen - numWeights) , iLen):\n",
    "    lookup = sortIndex[0][index]\n",
    "    weight =  round(modelLogit.coef_[0][lookup], 3)\n",
    "    print(vectorizer.get_feature_names()[sortIndex[0][index]], weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The size of the vocabulary for the training text data is 10569\n",
      "\n",
      "First 5 feature Names: ['000', '0000', '0011011001111000', '00pm', '012468']\n",
      "For C =  0.0001 Logistic regression accuracy: 0.74\n",
      "For C =  0.001 Logistic regression accuracy: 0.74\n",
      "For C =  0.01 Logistic regression accuracy: 0.741\n",
      "For C =  0.1 Logistic regression accuracy: 0.723\n",
      "For C =  0.5 Logistic regression accuracy: 0.702\n",
      "For C =  1.0 Logistic regression accuracy: 0.686\n",
      "For C =  2.0 Logistic regression accuracy: 0.684\n",
      "For C =  6.0 Logistic regression accuracy: 0.671\n",
      "For C =  10.0 Logistic regression accuracy: 0.662\n",
      "\n",
      "Top 25 Weighted Features:\n",
      "paying 0.001\n",
      "date 0.001\n",
      "father 0.001\n",
      "almost 0.001\n",
      "call 0.001\n",
      "thursday 0.001\n",
      "ve 0.001\n",
      "again 0.001\n",
      "check 0.001\n",
      "went 0.001\n",
      "last 0.001\n",
      "exchange 0.001\n",
      "tough 0.001\n",
      "request 0.001\n",
      "paycheck 0.001\n",
      "back 0.001\n",
      "bucks 0.001\n",
      "jpg 0.001\n",
      "sunday 0.001\n",
      "he 0.001\n",
      "rice 0.001\n",
      "bills 0.001\n",
      "wife 0.001\n",
      "her 0.002\n",
      "she 0.002\n"
     ]
    }
   ],
   "source": [
    "# vectorize the edited text\n",
    "reqTText = allTData[['request_id','request_text_edit_aware']]\n",
    "reqTText.set_index('request_id', inplace=True)\n",
    "#print(reqTText[:5])\n",
    "reqDText = allDData[['request_id','request_text_edit_aware']]\n",
    "reqDText.set_index('request_id', inplace=True)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "reqTVector = vectorizer.fit_transform(reqTText['request_text_edit_aware'])\n",
    "reqDVector = vectorizer.transform(reqDText['request_text_edit_aware'])\n",
    "\n",
    "print (\"\\nThe size of the vocabulary for the training text data is\", reqTVector.shape[1])\n",
    "print(\"\\nFirst 5 feature Names:\", vectorizer.get_feature_names()[1:6])\n",
    "\n",
    "# get the best regularization\n",
    "regStrength = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 6.0, 10.0]\n",
    "catCoef = []\n",
    "\n",
    "for c in regStrength:\n",
    "    modelLogit = LogisticRegression(penalty='l2', C=c)\n",
    "    modelLogit.fit(reqTVector, trainLabel)\n",
    "    logitScore = round(modelLogit.score(reqDVector, devLabel), 4)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "numWeights = 25\n",
    "\n",
    "\n",
    "# Fit a logistic regression on training data\n",
    "modelLogit = LogisticRegression(penalty='l2', C=.01)\n",
    "\n",
    "modelLogit.fit(reqTVector, trainLabel)\n",
    "\n",
    "indexArray = np.zeros(numWeights)\n",
    "sortIndex = np.argsort(modelLogit.coef_)\n",
    "iLen = len(sortIndex[0])\n",
    "print(\"\\nTop\", numWeights, \"Weighted Features:\")\n",
    "\n",
    "for index in range((iLen - numWeights) , iLen):\n",
    "    lookup = sortIndex[0][index]\n",
    "    weight =  round(modelLogit.coef_[0][lookup], 3)\n",
    "    print(vectorizer.get_feature_names()[sortIndex[0][index]], weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The size of the vocabulary for the training text data is 3839\n",
      "\n",
      "First 5 feature Names: ['00243364', '02', '08', '09', '10']\n",
      "For C =  0.0001 Logistic regression accuracy: 0.74\n",
      "For C =  0.001 Logistic regression accuracy: 0.74\n",
      "For C =  0.01 Logistic regression accuracy: 0.74\n",
      "For C =  0.1 Logistic regression accuracy: 0.735\n",
      "For C =  0.5 Logistic regression accuracy: 0.718\n",
      "For C =  1.0 Logistic regression accuracy: 0.711\n",
      "For C =  2.0 Logistic regression accuracy: 0.692\n",
      "For C =  6.0 Logistic regression accuracy: 0.672\n",
      "For C =  10.0 Logistic regression accuracy: 0.659\n",
      "\n",
      "Top 25 Weighted Features:\n",
      "mother 0.039\n",
      "ramen 0.04\n",
      "days 0.04\n",
      "kids 0.041\n",
      "was 0.041\n",
      "daughter 0.041\n",
      "couple 0.042\n",
      "rough 0.042\n",
      "city 0.042\n",
      "wife 0.043\n",
      "texas 0.045\n",
      "single 0.045\n",
      "will 0.05\n",
      "get 0.051\n",
      "forward 0.051\n",
      "help 0.053\n",
      "having 0.057\n",
      "friday 0.061\n",
      "make 0.063\n",
      "last 0.07\n",
      "tonight 0.072\n",
      "or 0.073\n",
      "until 0.074\n",
      "night 0.085\n",
      "little 0.096\n"
     ]
    }
   ],
   "source": [
    "# vectorize the request title\n",
    "reqTText = allTData[['request_id','request_title']]\n",
    "reqTText.set_index('request_id', inplace=True)\n",
    "#print(reqTText[:5])\n",
    "reqDText = allDData[['request_id','request_title']]\n",
    "reqDText.set_index('request_id', inplace=True)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "reqTVector = vectorizer.fit_transform(reqTText['request_title'])\n",
    "reqDVector = vectorizer.transform(reqDText['request_title'])\n",
    "\n",
    "print (\"\\nThe size of the vocabulary for the training text data is\", reqTVector.shape[1])\n",
    "print(\"\\nFirst 5 feature Names:\", vectorizer.get_feature_names()[1:6])\n",
    "\n",
    "# get the best regularization\n",
    "regStrength = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 6.0, 10.0]\n",
    "catCoef = []\n",
    "\n",
    "for c in regStrength:\n",
    "    modelLogit = LogisticRegression(penalty='l2', C=c)\n",
    "    modelLogit.fit(reqTVector, trainLabel)\n",
    "    logitScore = round(modelLogit.score(reqDVector, devLabel), 4)\n",
    "    print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "numWeights = 25\n",
    "\n",
    "\n",
    "# Fit a logistic regression on training data\n",
    "modelLogit = LogisticRegression(penalty='l2', C=.01)\n",
    "\n",
    "modelLogit.fit(reqTVector, trainLabel)\n",
    "\n",
    "indexArray = np.zeros(numWeights)\n",
    "sortIndex = np.argsort(modelLogit.coef_)\n",
    "iLen = len(sortIndex[0])\n",
    "print(\"\\nTop\", numWeights, \"Weighted Features:\")\n",
    "\n",
    "for index in range((iLen - numWeights) , iLen):\n",
    "    lookup = sortIndex[0][index]\n",
    "    weight =  round(modelLogit.coef_[0][lookup], 3)\n",
    "    print(vectorizer.get_feature_names()[sortIndex[0][index]], weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
