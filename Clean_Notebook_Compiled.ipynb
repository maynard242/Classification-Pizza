{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Acts of Pizza\n",
    "\n",
    "W207 Final Project\n",
    "\n",
    "Erika Lawrence, Leslie Teo, Jen Jen Chen, Geoff Stirling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_\"The universe is hilarious. Like, Venus is 900 degrees. I could tell you it melts lead. But that's not as fun as saying, 'You can cook a pizza on the windowsill in nine seconds.' And next time my fans eat pizza, they're thinking of Venus!\"_\n",
    "\n",
    "_- Neil deGrasse Tyson_ \n",
    "\n",
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setting Up & Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import Libraries ##\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pandas import *\n",
    "from pandas.io.json import json_normalize\n",
    "from vaderSentiment import vaderSentiment\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlxtend\n",
    "import scipy\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# SK-learn libraries for pre/processing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Miscellaneous libraries\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the normalized Data: (3040, 32)\n",
      "\n",
      "normalized data columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "## Get Data ##\n",
    "\n",
    "# Reference for data: https://www.kaggle.com/c/random-acts-of-pizza/data\n",
    "# Pull in the training and test data\n",
    "with open('data/train.json', encoding='utf-8') as data_file:\n",
    "    trainData = json.loads(data_file.read())   \n",
    "\n",
    "with open('data/test.json', encoding='utf-8') as data_file:\n",
    "    testData = json.loads(data_file.read())    \n",
    "\n",
    "# Create a dev data set \n",
    "devData = trainData[0:1000]\n",
    "trainData = trainData[1000:]\n",
    "\n",
    "# Show how the data looks in its original format\n",
    "#pprint(\"data in json format:\")\n",
    "#pprint(trainData[1])\n",
    "\n",
    "# Create a normalized view\n",
    "allTData = json_normalize(trainData)\n",
    "print(\"\\nSize of the normalized Data:\", allTData.shape)\n",
    "print(\"\\nnormalized data columns:\", list(allTData))\n",
    "\n",
    "allDData = json_normalize(devData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create subsets of data for analysis ###\n",
    "\n",
    "# Create a flat dataset without the subreddits list\n",
    "flatData = allTData.drop('requester_subreddits_at_request', 1)\n",
    "# Create a separate dataset with just subreddits, indexed on request id\n",
    "# We can creata a count vector on the words, run Naive Bayes against it, \n",
    "# and add the probabilities to our flat dataset\n",
    "subredTData = allTData[['request_id','requester_subreddits_at_request']]\n",
    "subredTData.set_index('request_id', inplace=True)\n",
    "\n",
    "subredDData= allDData[['request_id','requester_subreddits_at_request']]\n",
    "subredDData.set_index('request_id', inplace=True)\n",
    "\n",
    "# our training labels\n",
    "trainLabel = allTData['requester_received_pizza']\n",
    "\n",
    "devLabel = allDData['requester_received_pizza']\n",
    "\n",
    "# What do these look like?\n",
    "#print(list(flatData))\n",
    "print(subredTData.shape)\n",
    "#print(subredTData['requester_subreddits_at_request'][1])\n",
    "\n",
    "# Create a corpus of subreddits to vectorize\n",
    "trainCorpus = []\n",
    "rTCorpus = []\n",
    "rDCorpus = []\n",
    "\n",
    "for index in range(len(subredTData)):\n",
    "    trainCorpus.append(' '.join(subredTData['requester_subreddits_at_request'][index]))\n",
    "    rTCorpus.append(' '.join(subredTData['requester_subreddits_at_request'][index]))\n",
    "\n",
    "devCorpus = []\n",
    "for index in range(len(subredDData)):\n",
    "    devCorpus.append(' '.join(subredDData['requester_subreddits_at_request'][index]))\n",
    "    rDCorpus.append(' '.join(subredDData['requester_subreddits_at_request'][index]))\n",
    "\n",
    "# Baseline info\n",
    "print(\"Percent of people who got pizza:\", round(sum(trainLabel)/len(trainLabel),3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine all text sources into a single corpus\n",
    "fldTText = allTData[['request_id','request_text', 'request_text_edit_aware', 'request_title']]\n",
    "fldDText = allDData[['request_id','request_text', 'request_text_edit_aware', 'request_title']]\n",
    "\n",
    "trainCorpus = []\n",
    "for index in range(len(subredTData)):\n",
    "    a = ' '.join(subredTData['requester_subreddits_at_request'][index])\n",
    "    b = (a, fldTText['request_text'][index], fldTText['request_text_edit_aware'][index],\n",
    "        fldTText['request_title'][index])\n",
    "    trainCorpus.append(' '.join(b))\n",
    "\n",
    "devCorpus = []\n",
    "for index in range(len(subredDData)):\n",
    "    a = ' '.join(subredDData['requester_subreddits_at_request'][index])\n",
    "    b = (a, fldDText['request_text'][index], fldDText['request_text_edit_aware'][index],\n",
    "         fldDText['request_title'][index])\n",
    "    devCorpus.append(' '.join(b))\n",
    "\n",
    "# Print 3 examples  \n",
    "print (trainCorpus[:1])\n",
    "labels = trainLabel.astype(int)\n",
    "labels = list(labels)\n",
    "print(labels[:1])\n",
    "print('-'*75)\n",
    "\n",
    "print ('\\n' , devCorpus[:1])\n",
    "labels_dev = devLabel.astype(int)\n",
    "labels_dev = list(labels_dev)\n",
    "print(labels_dev[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Simple Feature Selection and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple Pre-Processing\n",
    "\n",
    "def data_preprocessor(s):\n",
    "    \"\"\"\n",
    "    Note: this function pre-processors data:\n",
    "    (1) removes non-alpha characters\n",
    "    (2) converts digits to 'number'\n",
    "    (3) regularizes spaces (although CountVectorizer ignores this unless they are part of words)\n",
    "    (4) reduces word size to n\n",
    "    \"\"\"\n",
    "\n",
    "    s = [re.sub(r'[?|$|.|!|@|\\n|(|)|<|>|_|-|,|\\']',r' ',s) for s in s] # strip out non-alpha numeric char, replace with space\n",
    "    s = [re.sub(r'\\d+',r'number ',s) for s in s] # convert digits to number\n",
    "    s = [re.sub(r' +',r' ',s) for s in s] # convert multiple spaces to single space\n",
    "    \n",
    "    # This sets word size to n=5\n",
    "    num = 5\n",
    "    def set_word(s):\n",
    "        temp = []\n",
    "        for s in s:\n",
    "            x = s.split()\n",
    "            z = [elem[:num] for elem in x]\n",
    "            z = ' '.join(z)\n",
    "            temp.append(z)       \n",
    "        return temp\n",
    "    \n",
    "    s = set_word(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 BiGrams \n",
    "After trying unigram and trigram vectorizers, the best results were found using bigrams in logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try it with bigrams \n",
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word', ngram_range=(2, 2), \n",
    "                                token_pattern=r'\\b\\w+\\b', min_df=1,stop_words = stop)\n",
    "# Transform the corpus into vectorized trigrams\n",
    "tVector = vectorizer.fit_transform(trainCorpus)\n",
    "dVector = vectorizer.transform(data_preprocessor(devCorpus))\n",
    "# How does it look?\n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", tVector.shape[1])\n",
    "print (\"First 5 feature Names:\", vectorizer.get_feature_names()[1:6], \"\\n\")\n",
    "# Use the preprocessor and do the same\n",
    "vectorizer_p = CountVectorizer(analyzer = 'word', ngram_range=(2, 2), \n",
    "                                  token_pattern=r'\\b\\w+\\b', min_df=1,stop_words = stop)\n",
    "tVector_p = vectorizer_p.fit_transform(data_preprocessor(trainCorpus))\n",
    "dVector_p = vectorizer_p.transform(data_preprocessor(devCorpus))\n",
    "# How does the pre-processed vector look?\n",
    "print ('\\nRaw data:')\n",
    "print (\"The size of the vocabulary for the training text data is\", tVector_p.shape[1])\n",
    "print (\"First 5 feature Names:\", vectorizer_p.get_feature_names()[1:6], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try PCA\n",
    "\n",
    "# k = 250 principal components    \n",
    "n_comp = 250    \n",
    "pca_mod = PCA(n_components = n_comp)\n",
    "# PCA requires dense vectors:\n",
    "tDense = tVector_p.todense()\n",
    "# Get the principal components for the dense vector\n",
    "pca_mod.fit(tDense)\n",
    "# Find the fraction of the variance explained by each component\n",
    "pcaVarRatio =  pca_mod.explained_variance_ratio_ \n",
    "pcaCumVarRatio =  np.cumsum(pca_mod.explained_variance_ratio_)\n",
    "   \n",
    "# Plot the fraction of variance explained by each component, and the cumulative percent \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(range(len(pcaVarRatio)), pcaVarRatio, c = 'g', marker=\"s\", label='Fraction')\n",
    "ax1.scatter(range(len(pcaVarRatio)), pcaCumVarRatio, c = 'purple',marker=\"o\", \n",
    "            label='Cumulative')\n",
    "plt.legend(loc='upper left');\n",
    "ax1.set_title('Fraction of Total Variance for k = 1 to 250');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3:  Status (Upvotes-Downvotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETER 1:  DIFFERENCE IN UPVOTES-DOWNVOTES (from time of request to time of retrieval) ##\n",
    "\n",
    "# Training data\n",
    "\n",
    "# Create separate database of desired columns\n",
    "diffTrequest = allTData['requester_upvotes_minus_downvotes_at_request']\n",
    "diffTretrieval = allTData['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "\n",
    "# Calculate upvote-downvote difference and add back into database\n",
    "allTData['request_to_retrieval_upvotes_minus_downvotes'] = diffTretrieval - diffTrequest\n",
    "\n",
    "\n",
    "# Development data (same process as above)\n",
    "diffDrequest = allDData['requester_upvotes_minus_downvotes_at_request']\n",
    "diffDretrieval = allDData['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "allDData['request_to_retrieval_upvotes_minus_downvotes'] = diffDretrieval - diffDrequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CONVERT UPVOTES-DOWNVOTES SUBSETS INTO Z-SCORES FOR STANDARDIZATION ##\n",
    "\n",
    "# Z-scores for training data\n",
    "\n",
    "# Retrieve appropriate columns\n",
    "statusTtrain = allTData[['requester_upvotes_minus_downvotes_at_request','requester_upvotes_minus_downvotes_at_retrieval',\n",
    "                          'request_to_retrieval_upvotes_minus_downvotes']].copy()\n",
    "statusTtrain.columns = ['Request', 'Retrieval', 'Request-Retrieval']\n",
    "# Convert into Z-scores\n",
    "for col in statusTtrain:\n",
    "    zscore_col = col + \" Z-score\"\n",
    "    statusTtrain[zscore_col] = (statusTtrain[col] - statusTtrain[col].mean() / statusTtrain[col].std(ddof=0))\n",
    "\n",
    "# Z-scores for development data(same process as above)\n",
    "statusDtrain = allDData[['requester_upvotes_minus_downvotes_at_request','requester_upvotes_minus_downvotes_at_retrieval',\n",
    "                          'request_to_retrieval_upvotes_minus_downvotes']].copy()\n",
    "statusDtrain.columns = ['Request', 'Retrieval', 'Request-Retrieval']\n",
    "\n",
    "for col in statusDtrain:\n",
    "    zscore_dev_col = col + \" Z-score\"\n",
    "    statusDtrain[zscore_dev_col] = (statusDtrain[col] - statusDtrain[col].mean() / statusDtrain[col].std(ddof=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.4:  Status (Upvotes-Downvotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETER 2:  TIMEDATE STAMP OF REDDIT POST ##\n",
    "\n",
    "\n",
    "unix = allTData[['unix_timestamp_of_request_utc', 'requester_received_pizza']].copy()\n",
    "\n",
    "# Convert from unix > datetime\n",
    "unix['Datetime'] = pandas.to_datetime(unix['unix_timestamp_of_request_utc'], unit='s')\n",
    "unix['Hour'] = unix['Datetime'].dt.hour\n",
    "display(unix.head(5))\n",
    "\n",
    "# set datetime as index\n",
    "unix_index = unix.set_index('Datetime')\n",
    "unix_index = unix_index.drop(['unix_timestamp_of_request_utc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot pizzas by month \n",
    "%matplotlib inline\n",
    "unix_index_base = unix_index.drop(['Hour'], axis=1)\n",
    "month_pizzas = unix_index_base.resample('M').sum()\n",
    "\n",
    "month_pizzas_bar = month_pizzas.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TIME ANALYSIS - HOUR OF REDDIT POST\n",
    "\n",
    "# Removed first few months to minimize skew\n",
    "hourly = unix_index['10/2012':'9/2013'].groupby('Hour')['requester_received_pizza'].sum()\n",
    "print(\"Statistics of pizza success by hour\\n\")\n",
    "print(hourly.describe())\n",
    "print(\"\\n\\nPizza success by hour of the day\")\n",
    "hourly.plot('area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.5:  vaderSentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titles = allTData['request_title']\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(data = allTData)\n",
    "\n",
    "df = df[['request_title', 'requester_received_pizza']]\n",
    "\n",
    "\n",
    "titles = allTData['request_title']\n",
    "\n",
    "def print_sentiment_scores(sentence):\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    snt['compund']\n",
    "    #print(\"{:-<40} {}\".format(sentence, str(snt)))\n",
    "    #print(snt['compound'])\n",
    "    \n",
    "scores = []\n",
    "\n",
    "for title in titles:\n",
    "    scores.append(analyser.polarity_scores(title)['compound'])\n",
    "\n",
    "#print(scores[1:10])\n",
    "\n",
    "df[\"Vader Scores\"] = scores\n",
    "\n",
    "#df.head(10)\n",
    "\n",
    "df = df.drop('request_title', axis = 1)\n",
    "\n",
    "print(df)\n",
    "\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titles = allDData['request_title']\n",
    "\n",
    "df_d = pd.DataFrame(data = allDData)\n",
    "\n",
    "df_d = df_d[['request_title', 'requester_received_pizza']]\n",
    "\n",
    "def print_sentiment_scores(sentence):\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    snt['compund']\n",
    "    #print(\"{:-<40} {}\".format(sentence, str(snt)))\n",
    "    #print(snt['compound'])\n",
    "    \n",
    "scores_d = []\n",
    "\n",
    "for title in titles:\n",
    "    scores_d.append(analyser.polarity_scores(title)['compound'])\n",
    "\n",
    "df_d[\"Vader Scores\"] = scores_d\n",
    "\n",
    "df_d = df_d.drop('request_title', axis = 1)\n",
    "\n",
    "print(df_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tTitles = allTData['request_title']\n",
    "dTitles = allDData['request_title']\n",
    "\n",
    "titleTSentiment = []\n",
    "titleDSentiment = []\n",
    "\n",
    "for title in tTitles:\n",
    "    snt = analyser.polarity_scores(title)\n",
    "    compoundScore = snt['compound']\n",
    "    titleTSentiment.append(compoundScore)\n",
    "\n",
    "titleTSentiment = pd.DataFrame(titleTSentiment)\n",
    "    \n",
    "for title in dTitles:\n",
    "    snt = analyser.polarity_scores(title)\n",
    "    compoundScore = snt['compound']\n",
    "    titleDSentiment.append(compoundScore)\n",
    "\n",
    "titleDSentiment = pd.DataFrame(titleDSentiment)\n",
    "\n",
    "C = 100\n",
    "modelLogit = LogisticRegression(penalty = 'l2', C = C)\n",
    "\n",
    "trainLabel = allTData['requester_received_pizza']\n",
    "devLabel = allDData['requester_received_pizza']\n",
    "\n",
    "modelLogit.fit(titleTSentiment,trainLabel)\n",
    "score_rep(devLabel,modelLogit.predict(titleDSentiment),'Logistic Regression, C = 0.001')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Models\n",
    "\n",
    "### 3.1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression: bigrams\n",
    "# Fit a logistic regression model to our bigrams, using l2 regularization and C = .5\n",
    "c = .01\n",
    "modelLogit = LogisticRegression(penalty='l2', C=c)\n",
    "modelLogit.fit(tVector, trainLabel)\n",
    "\n",
    "# Print out the classification report\n",
    "print(\"Classification report raw bigrams:\\n\")\n",
    "devPred = modelLogit.predict(dVector)\n",
    "print(classification_report(devLabel, devPred))\n",
    "\n",
    "# Print out the accuracy scode   \n",
    "logitScore = round(modelLogit.score(dVector, devLabel), 4)\n",
    "print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)\n",
    "\n",
    "# Fit the pre-processed data\n",
    "modelLogit_p = LogisticRegression(penalty='l2', C=c)\n",
    "modelLogit_p.fit(tVector_p, trainLabel)\n",
    "\n",
    "# Print out the classification report\n",
    "print(\"\\nClassification report processed bigrams:\\n\")\n",
    "devPred = modelLogit_p.predict(dVector_p)\n",
    "print(classification_report(devLabel, devPred))\n",
    "    \n",
    "# Print the accuracy score\n",
    "logitScore = round(modelLogit_p.score(dVector_p, devLabel), 4)\n",
    "print(\"For C = \", c, \"Logistic regression accuracy:\", logitScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2: Ensemble Model on PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an ensemble model on the PCA features\n",
    "# Get the training features\n",
    "pca_tran=pca_mod.transform(tDense)  \n",
    "# Convert our dev features to be dense\n",
    "dDense = dVector_p.todense()\n",
    "# Get the dev features\n",
    "pca_devTran=pca_mod.transform(dDense)  \n",
    "\n",
    "# Set up lr_1\n",
    "lr_1 = LogisticRegression(penalty='l2', C=0.01)\n",
    "# Set up lr_2\n",
    "lr_2 = LogisticRegression(penalty='l2', C=0.1)\n",
    "# Set up lr_3\n",
    "lr_3 = LogisticRegression(penalty='l2', C=0.5)\n",
    "\n",
    "# Set up ensemble of the models\n",
    "clf = EnsembleVoteClassifier(clfs=[lr_1, lr_2, lr_3], \n",
    "                             voting='soft', weights=[1,1, 1])\n",
    "\n",
    "# Fit training data\n",
    "clf.fit(pca_tran,trainLabel)\n",
    "  \n",
    "# Probabilities, predictions\n",
    "devProb = clf.predict_proba(pca_devTran)\n",
    "devPred = clf.predict(pca_devTran)\n",
    "\n",
    "acc = sum(devPred == devLabel)/len(devLabel)\n",
    "print(\"Logistic regression accuracy:\", acc)\n",
    "print(classification_report(devLabel, devPred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3: Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group scoring functions\n",
    "\n",
    "def roc_curve1(y_true, y_pred_prob):\n",
    "    \"\"\"This function plots the ROC curve\n",
    "    Inputs: y_true, correct label\n",
    "            y_pred_prob, predicted probabilities\n",
    "    \"\"\"\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_pred_prob)\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def score_rep(y_true, y_pred, desc):\n",
    "    \"\"\"Function to print out comprehensive report for classification test\n",
    "    Inputs: y_true, correct label\n",
    "            y_pred, predicted label from model\n",
    "            desc, description of model\n",
    "    Output: classification report\n",
    "    \"\"\"\n",
    "    print(desc)\n",
    "    print(\"-\"*75)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_true, y_pred))\n",
    "    print(\"Area under curve of ROC: \", metrics.roc_auc_score(y_true, y_pred))\n",
    "    print(\"Classification report:\\n\")\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(\"-\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4: Status & Hour LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# FOR STATUS AND TIME BY HOUR\n",
    "lr = LogisticRegression(penalty='l2', C=0.01)\n",
    "\n",
    "train_labels = np.asarray(allTData['requester_received_pizza'])\n",
    "dev_labels = np.asarray(allDData['requester_received_pizza'])\n",
    "\n",
    "# setup training and development datasets\n",
    "# train\n",
    "stathourTData = pandas.concat([statusTtrain['Request-Retrieval Z-score'], allTData['unix_timestamp_of_request_utc']], axis=1, \n",
    "                              keys=['Request-Retrieval Z-score', 'unix_timestamp_of_request_utc'])\n",
    "stathourTData['Datetime'] = pandas.to_datetime(stathourTData['unix_timestamp_of_request_utc'], unit='s')\n",
    "stathourTData['Hour'] = stathourTData['Datetime'].dt.hour\n",
    "stathourTData_ = stathourTData.drop(['Datetime', 'unix_timestamp_of_request_utc'], axis=1)\n",
    "# display(stathourTData.head(5))\n",
    "\n",
    "# development\n",
    "stathourDData = pandas.concat([statusDtrain['Request-Retrieval Z-score'], allDData['unix_timestamp_of_request_utc']], axis=1, \n",
    "                              keys=['Request-Retrieval Z-score', 'unix_timestamp_of_request_utc'])\n",
    "stathourDData['Datetime'] = pandas.to_datetime(stathourDData['unix_timestamp_of_request_utc'], unit='s')\n",
    "stathourDData['Hour'] = stathourDData['Datetime'].dt.hour\n",
    "stathourDData_ = stathourDData.drop(['Datetime', 'unix_timestamp_of_request_utc'], axis=1)\n",
    "# display(stathourDData.head(5))\n",
    "\n",
    "# fit LR models\n",
    "train_stathour_model = lr.fit(stathourTData_, train_labels)\n",
    "dev_stathour_labels = train_stathour_model.predict(stathourDData_)\n",
    "stathour_score = train_stathour_model.score(stathourDData_, dev_labels)\n",
    "stathour_F1score = metrics.f1_score(dev_stathour_labels, dev_labels)\n",
    "\n",
    "print(\"\\nScoring for Request-retrieval Z-scores and Hour\")\n",
    "print(\"Accuracy:\", stathour_score)\n",
    "print(\"F1 score:\", stathour_F1score)\n",
    "roc_curve1(dev_labels, train_stathour_model.predict_proba(stathourDData_)[:,0])\n",
    "score_rep(dev_labels, dev_stathour_labels,\"Logistic Regression, C = 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieve probabilities from text model where training label is True\n",
    "text_model = modelLogit.fit(tVector_p, trainLabel)\n",
    "prob = text_model.predict_proba(dVector_p)\n",
    "prob_true = [float(y) for x,y in prob]\n",
    "prob_true = pandas.DataFrame(prob_true)\n",
    "prob_false = [float(x) for x,y in prob]\n",
    "prob_false = pandas.DataFrame(prob_false)\n",
    "\n",
    "# add as a parameter to status/hour dataframe\n",
    "combinedTData = pandas.concat([stathourTData_, prob_true, prob_false], axis = 1)\n",
    "combinedDData = stathourDData_\n",
    "\n",
    "# fit LR models\n",
    "train_comb_model = lr.fit(combinedTData, train_labels)\n",
    "dev_comb_labels = train_train_comb_model.predict(combinedDData)\n",
    "comb_score = train_comb_model.score(combinedDData, dev_labels)\n",
    "comb_F1score = metrics.f1_score(dev_comb_labels, dev_labels)\n",
    "\n",
    "print(\"\\nScoring for Request-retrieval Z-scores and bucketed Hour:\")\n",
    "print(\"Accuracy:\", comb_score)\n",
    "print(\"F1 score:\", comb_F1score)\n",
    "roc_curve1(dev_labels, train_comb_model.predict_proba(combinedDData)[:,0])\n",
    "score_rep(dev_labels, dev_comb_labels,\"Logistic Regression, C = 0.01\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
