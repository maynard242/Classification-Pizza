{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project\n",
    "Erika, Jen Jen, Geoff, Leslie\n",
    "\n",
    "(In Python 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 3/12 \n",
    "\n",
    "Outline:\n",
    "\n",
    "* Data munging  \n",
    "* Simple Feature Selection\n",
    "* Basline models\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 Loading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import Libraries ##\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pandas import *\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.feature_extraction.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data in json format:'\n",
      "{'giver_username_if_known': 'N/A',\n",
      " 'number_of_downvotes_of_request_at_retrieval': 2,\n",
      " 'number_of_upvotes_of_request_at_retrieval': 2,\n",
      " 'post_was_edited': False,\n",
      " 'request_id': 't3_yemx8',\n",
      " 'request_number_of_comments_at_retrieval': 1,\n",
      " 'request_text': 'My boyfriend and I live in Saint Augustine, Florida and have '\n",
      "                 'been having a rough time financially the past few months.  '\n",
      "                 \"In and out of various jobs, we've had to survive off of \"\n",
      "                 'coscto sized ramen packs, and pasta and olive oil.  I '\n",
      "                 'applied for food stamps a couple days ago, and am waiting to '\n",
      "                 \"hear back from them.  It's getting a little trite, and we're \"\n",
      "                 \"quite hungry tonight, a hot pizza would be a delight.  We'll \"\n",
      "                 'happily pay it forward in the future.  Much love.',\n",
      " 'request_text_edit_aware': 'My boyfriend and I live in Saint Augustine, '\n",
      "                            'Florida and have been having a rough time '\n",
      "                            'financially the past few months.  In and out of '\n",
      "                            \"various jobs, we've had to survive off of coscto \"\n",
      "                            'sized ramen packs, and pasta and olive oil.  I '\n",
      "                            'applied for food stamps a couple days ago, and am '\n",
      "                            \"waiting to hear back from them.  It's getting a \"\n",
      "                            \"little trite, and we're quite hungry tonight, a \"\n",
      "                            \"hot pizza would be a delight.  We'll happily pay \"\n",
      "                            'it forward in the future.  Much love.',\n",
      " 'request_title': '[Request]  Saint Augustine, US.  Boyfriend and I have no '\n",
      "                  'money till next week, and are awaiting food stamps '\n",
      "                  'approval.',\n",
      " 'requester_account_age_in_days_at_request': 444.6098148148148,\n",
      " 'requester_account_age_in_days_at_retrieval': 919.923113425926,\n",
      " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
      " 'requester_days_since_first_post_on_raop_at_retrieval': 475.27163194444444,\n",
      " 'requester_number_of_comments_at_request': 99,\n",
      " 'requester_number_of_comments_at_retrieval': 182,\n",
      " 'requester_number_of_comments_in_raop_at_request': 0,\n",
      " 'requester_number_of_comments_in_raop_at_retrieval': 10,\n",
      " 'requester_number_of_posts_at_request': 6,\n",
      " 'requester_number_of_posts_at_retrieval': 16,\n",
      " 'requester_number_of_posts_on_raop_at_request': 0,\n",
      " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
      " 'requester_number_of_subreddits_at_request': 38,\n",
      " 'requester_received_pizza': False,\n",
      " 'requester_subreddits_at_request': ['AskReddit',\n",
      "                                     'Guitar',\n",
      "                                     'Jazz',\n",
      "                                     'Music',\n",
      "                                     'NSFW_GIF',\n",
      "                                     'Psychonaut',\n",
      "                                     'RoomPorn',\n",
      "                                     'StAugustine',\n",
      "                                     'TwoXChromosomes',\n",
      "                                     'WTF',\n",
      "                                     'YouShouldKnow',\n",
      "                                     'atheism',\n",
      "                                     'aww',\n",
      "                                     'bakedart',\n",
      "                                     'catpictures',\n",
      "                                     'cats',\n",
      "                                     'crochet',\n",
      "                                     'dubstep',\n",
      "                                     'ents',\n",
      "                                     'entwives',\n",
      "                                     'food',\n",
      "                                     'funny',\n",
      "                                     'gonewild',\n",
      "                                     'hiphopheads',\n",
      "                                     'listentothis',\n",
      "                                     'meetup',\n",
      "                                     'offbeat',\n",
      "                                     'pics',\n",
      "                                     'realpics',\n",
      "                                     'self',\n",
      "                                     'sex',\n",
      "                                     'tattoos',\n",
      "                                     'treecomics',\n",
      "                                     'treemusic',\n",
      "                                     'trees',\n",
      "                                     'videos',\n",
      "                                     'vinyl',\n",
      "                                     'zombies'],\n",
      " 'requester_upvotes_minus_downvotes_at_request': 278,\n",
      " 'requester_upvotes_minus_downvotes_at_retrieval': 664,\n",
      " 'requester_upvotes_plus_downvotes_at_request': 378,\n",
      " 'requester_upvotes_plus_downvotes_at_retrieval': 942,\n",
      " 'requester_user_flair': None,\n",
      " 'requester_username': 'Faroffhighways',\n",
      " 'unix_timestamp_of_request': 1345254263.0,\n",
      " 'unix_timestamp_of_request_utc': 1345250663.0}\n",
      "\n",
      "Size of the normalized Data: (3040, 32)\n",
      "\n",
      "normalized data columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "## Get Data ##\n",
    "\n",
    "#reference on data: https://www.kaggle.com/c/random-acts-of-pizza/data\n",
    "# pull in the training and test data\n",
    "#with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/train.json', encoding='utf-8') as data_file:\n",
    "with open('C:/Users/levi/Documents/GitHub/W207_Proj/W207_Proj/data/train.json', encoding='utf-8') as data_file:\n",
    "    trainData = json.loads(data_file.read())   \n",
    "#with open('/Users/erikaananda/Documents/MIDS/W207/Final Project/data/test.json', encoding='utf-8') as data_file:\n",
    "with open('C:/Users/levi/Documents/GitHub/W207_Proj/W207_Proj/data/test.json', encoding='utf-8') as data_file:\n",
    "    testData = json.loads(data_file.read())    \n",
    "\n",
    "# create a dev data set \n",
    "devData = trainData[0:1000]\n",
    "trainData = trainData[1000:]\n",
    "\n",
    "# show how the data looks in its original format\n",
    "pprint(\"data in json format:\")\n",
    "pprint(trainData[1])\n",
    "\n",
    "# create a normalized view\n",
    "allData = json_normalize(trainData)\n",
    "print(\"\\nSize of the normalized Data:\", allData.shape)\n",
    "print(\"\\nnormalized data columns:\", list(allData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "(3040, 1)\n",
      "My boyfriend and I live in Saint Augustine, Florida and have been having a rough time financially the past few months.  In and out of various jobs, we've had to survive off of coscto sized ramen packs, and pasta and olive oil.  I applied for food stamps a couple days ago, and am waiting to hear back from them.  It's getting a little trite, and we're quite hungry tonight, a hot pizza would be a delight.  We'll happily pay it forward in the future.  Much love.\n"
     ]
    }
   ],
   "source": [
    "## Create subsets of data for analysis ###\n",
    "\n",
    "# create a flat dataset without the subreddits list\n",
    "flatData = allData.drop('requester_subreddits_at_request', 1)\n",
    "# create a separate dataset with just subreddits, indexed on request id\n",
    "# we can creata a count vector on the words, run Naive Bayes against it, \n",
    "# and add the probabilitiesL to our flat dataset\n",
    "subredData = allData[['request_id','request_text_edit_aware']]\n",
    "subredData.set_index('request_id', inplace=True)\n",
    "# our training labels\n",
    "trainLabel = allData['requester_received_pizza']\n",
    "\n",
    "# what do these look like?\n",
    "print(list(flatData))\n",
    "print(subredData.shape)\n",
    "print(subredData['request_text_edit_aware'][1])\n",
    "\n",
    "# create a corpus of subreddits to vectorize\n",
    "corpus = []\n",
    "for index in range(len(subredData)):\n",
    "    corpus.append(' '.join(subredData['request_text_edit_aware'][index]))\n",
    "    \n",
    "## NOTE Changed requester_subedit_at_request to request_text_edit_aware in lines 8, 16, 21\n",
    "## Couldn't get above to work for me\n",
    "\n",
    "## Add for dev data\n",
    "allData_dev = json_normalize(devData)\n",
    "subredData_dev = allData_dev[['request_id','request_text_edit_aware']]\n",
    "subredData_dev.set_index('request_id', inplace=True)\n",
    "devLabel = allData_dev['requester_received_pizza']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary for the training data is 10569\n",
      "First 10 feature Names: ['000', '0000', '0011011001111000', '00pm', '012468', '024856', '0273', '04', '05ffr']\n"
     ]
    }
   ],
   "source": [
    "## Analyze ##\n",
    "\n",
    "corpus = list(subredData['request_text_edit_aware']) # I think this works DOUBLE CHECK\n",
    "corpus_dev = list(subredData_dev['request_text_edit_aware'])\n",
    "\n",
    "# create a train vector\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "trainVector = vectorizer.fit_transform(corpus)\n",
    "print (\"The size of the vocabulary for the training data is\", trainVector.shape[1])\n",
    "print(\"First 10 feature Names:\", vectorizer.get_feature_names()[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Section 2. Simple Feature Selection and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\levi\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\levi\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"So it's been a while since it's happened, but yeah. Just got dumped by a girl I like... My brother has used up pretty much everything in the freezer. I would really appreciate a pizza right now...\", \"My boyfriend and I live in Saint Augustine, Florida and have been having a rough time financially the past few months.  In and out of various jobs, we've had to survive off of coscto sized ramen packs, and pasta and olive oil.  I applied for food stamps a couple days ago, and am waiting to hear back from them.  It's getting a little trite, and we're quite hungry tonight, a hot pizza would be a delight.  We'll happily pay it forward in the future.  Much love.\", \"I seriously love buffalo chicken pizza. Like, straight up addicted. There's a local pizzeria that delivers and they make the best buffalo chicken pizza I've ever had; however if you dont feel safe with that or have a Giftcard to a chain I could care less, I just want a buffalo chicken pizza soooo bad! \"]\n",
      "[0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "---------------------------------------------------------------------------\n",
      "['Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated', 'I spent the last money I had on gas today. Im broke until next Thursday :(', \"My girlfriend decided it would be a good idea to get off at Perth bus station when she was coming to visit me and has since had to spend all her money on a taxi to get to me here in Dundee. Any chance some kind soul would get us some pizza since we don't have any cash anymore?\"]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Testing the raw data\n",
    "\n",
    "print(corpus[:3])\n",
    "labels = trainLabel.astype(int)\n",
    "labels = list(labels)\n",
    "print(labels[:20])\n",
    "\n",
    "print('-'*75)\n",
    "\n",
    "print(corpus_dev[:3])\n",
    "labels_dev = devLabel.astype(int)\n",
    "labels_dev = list(labels_dev)\n",
    "print(labels_dev[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple Pre-Processing\n",
    "\n",
    "def data_preprocessor(s):\n",
    "    \"\"\"\n",
    "    Note: this function pre-processors data:\n",
    "    (i) to lower case (although CountVectorizer has an option for that)\n",
    "    (ii) removes non-alpha characters\n",
    "    (iii) converts digits to 'number'\n",
    "    (iv) regularizes spaces (although CountVectorizer ignores this unless they are part of words)\n",
    "    (v) reduces word size to n\n",
    "    \"\"\"\n",
    "    \n",
    "    s = [s.lower() for s in s] # change to lower case ALTHOUGH Countvectorizer has option\n",
    "    s = [re.sub(r'[?|$|.|!|@|\\n|(|)|<|>|_|-|,|\\']',r' ',s) for s in s] # strip out non-alpha numeric char, replace with space\n",
    "    s = [re.sub(r'\\d+',r'number ',s) for s in s] # convert digits to number\n",
    "    s = [re.sub(r' +',r' ',s) for s in s] # convert multiple spaces to single space\n",
    "    \n",
    "    # This sets word size to n=10\n",
    "    def set_word(s):\n",
    "        temp = []\n",
    "        for s in s:\n",
    "            x = s.split()\n",
    "            z = [elem[:10] for elem in x]\n",
    "            z = ' '.join(z)\n",
    "            temp.append(z)       \n",
    "        return temp\n",
    "    \n",
    "    s = set_word(s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, strip_accents='unicode',stop_words='english')\n",
    "\n",
    "# Preprocess data\n",
    "train_data_v = vectorizer.fit_transform(data_preprocessor(corpus)).toarray()\n",
    "dev_data_v = vectorizer.transform(data_preprocessor(corpus_dev)).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression f1-score with C = 0.1:\n",
      "0.73\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', multi_class='multinomial', solver='sag', tol=0.01, C=0.1)\n",
    "clf.fit(train_data_v,labels)\n",
    "test_predicted_labels = clf.predict(dev_data_v) \n",
    "\n",
    "print ('\\nLogistic Regression f1-score with C = 0.1:' )\n",
    "print (metrics.f1_score(labels_dev,test_predicted_labels,average='micro')) # Should really use dev data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multinomial NB f1-score with alpha = 0.010:\n",
      "0.701\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "clf = MultinomialNB(alpha=alpha)\n",
    "clf.fit(train_data_v, labels)\n",
    "test_predicted_labels = clf.predict(dev_data_v) \n",
    "\n",
    "print ('\\nMultinomial NB f1-score with alpha = %1.3f:' %alpha )\n",
    "print (metrics.f1_score(labels_dev,test_predicted_labels,average='micro'))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
